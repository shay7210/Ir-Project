from pyspark.sql.functions import col, count
from graphframes import GraphFrame

# --- CONFIGURATION ---
bucket_name = 'wikipidia_ir_project'
# Uses your specific file location
input_path = f"gs://{bucket_name}/*.parquet"

# --- HELPER FUNCTION ---
def generate_graph(pages):
  ''' Compute the directed graph generated by wiki links.
  Returns: edges (src, dst), vertices (id)
  '''
  # Extract edges from anchor text: (source_doc_id, target_doc_id)
  edges = pages.flatMap(lambda row: [(row.id, anchor[0]) for anchor in row.anchor_text]).distinct()
  # Vertices are just the doc IDs
  vertices = edges.flatMap(lambda x: x).distinct().map(lambda x: (x,))
  return edges, vertices

# --- EXECUTION ---
print("--- Starting PageRank Computation ---")
print(f"Reading data from: {input_path}")
parquetFile = spark.read.parquet(input_path)

# 1. Prepare Data
pages_links = parquetFile.select("id", "anchor_text").rdd
edges, vertices = generate_graph(pages_links)

# 2. Create DataFrames for GraphFrames
edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')
verticesDF = vertices.toDF(['id']).repartition(124, 'id')

# 3. Run PageRank (using GraphFrames)
g = GraphFrame(verticesDF, edgesDF)

# Standard parameters: 0.15 reset probability, 6 iterations (as per assignment)
pr_results = g.pageRank(resetProbability=0.15, maxIter=6)

# 4. Save Results
pr = pr_results.vertices.select("id", "pagerank")
pr = pr.sort(col('pagerank').desc())

# Save to your bucket
output_path = f'gs://{bucket_name}/pageRank'
print(f"Saving results to {output_path}...")
pr.repartition(1).write.csv(output_path, compression="gzip")

print(f"DONE! PageRank saved to {output_path}")